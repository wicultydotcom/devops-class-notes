Installing Kubernetes Using Kubeadm
===========================================

Creating VM in Google cloud
##################
1. In the Google cloud console, click on three lines (top, left)
2. Go to 'Compute Engine' --> VM Instances
3. Click on 'Create Instalnce'
4. Give a machine name in 'Name' field (ex:master)
5. Select machine size under 'Series' as 'N2' (Need minimum 2GB RAM & 2vCPU)
6. On the left menu, select 'OS and Storage'
7. To select OS for the VM, Under 'Operating system and storage', click on 'Change'
8. Under 'Operating System' drop-down, select 'ubuntu'
9. Under 'Version', select 'Ubuntu 24.10 (x86/64, amd64 oracular image ....)'
   (basically, we need ubuntu 24.10 with x86/64 bit processor architecture)
10. Click on 'select' butoon
11. On left menu, click on 'Networking' and select 'Allow HTTP trafic' and 'Allow HTTPS trafic'
12. And finally, click on 'create' button at the bottom 



Topic: Setting up Control-plane/Master node
#############################################
1)
# Install container runtime - containerd
Follow this source.
Source:
https://kubesimplify.com/kubernetes-containerd-setup


2)
# Install Kubeadm, Kubelet & Kubectl
Follow this source or below commands.	

Source:
https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/	

#3) Initialize a new Kubernetes control-plane node

$ sudo kubeadm init
NOTE: Save this output as it's required to add worker nodes.

        
4)
# As per the instruction from 'kubeadm init' command output, To make kubectl work for your non-root user, run these commands.
  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

#5) Verify if cluster is initialized succussfuly
$ kubectl get nodes
O/P:
	NAME STATUS ROLES AGE VERSION
	node1 NotReady master 2m43s v1.12.1


#9) Run the following kubectl command to find the reason why the cluster STATUS is showing as NotReady.
	- This command shows all Pods in all namespaces - this includes system Pods in the system (kube-system) namespace.
	- As we can see, none of the coredns Pods are running
	- This is preventing the cluster from entering the Ready state, and is happening because we haven’t created the Pod network yet.
O/P:
$ kubectl get pods --all-namespaces
NAMESPACE NAME READY STATUS RESTARTS AGE
kube-system coredns-...vt 0/1 Pending 0 8m33s
kube-system coredns-...xw 0/1 Pending 0 8m33s
kube-system etcd... 1/1 Running 0 7m46s
kube-system kube-api... 1/1 Running 0 7m36s

#7) Create Pod Network. You must install a pod network add-on so that your pods can communicate with each other. (As per kubeadm init output)

Source: https://github.com/rajch/weave#using-weave-on-kubernetes [Take this link from $ kubeadm init output]

Run below command to install a Pod network add-on

$ kubectl apply -f https://reweave.azurewebsites.net/k8s/v1.28/net.yaml


#8) Check if the status of Master is changed from 'NotReady' to 'Ready'
$ kubectl get nodes
NAME STATUS ROLES AGE VERSION
node1 Ready master 3m51s v1.12.1

GREAT - the cluster is ready and all dns system pods are now working. Master is ready now.
		Now that the cluster is up and running, it’s time to add some worker-nodes.


#
Topic: Worker Node Setup & Joining to the Master:
#############################################
1
# Create a worker node machine in GCP / AWS cloud platform.

2
# Install kubeadm, Kubelet, Kubectl
https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/

3
# Install container runtime that is "containerd"
Follow this source.
Source:
https://kubesimplify.com/kubernetes-containerd-setup 

4
# To join Kubernetes worker node with control-plane node, run below command as root user.

Note: Below command/token will be different for your Control-plane/Master node. Use the one which you have copied at step:3.

$ sudo kubeadm join 10.128.0.18:6443 --token 9ril81.t4k4sqh1ionqv1om \
    --discovery-token-ca-cert-hash sha256:de57d9e08877db501a8b503db3ee91596f8f5657878c3087bc0343ece7df3eb2

NOTE: above token is valid only for 24 hours i.e same token you can't use to join worker nodes after 24 hours. If you are joining worker node to Master after 24 hours, use below command to create a new token.
$ kubeadm token create --print-join-command (Use this newly generated command for joining the worker-node)

	
# Verify node Join (Run below in Control-plane node)

$ kubectl get nodes
NAME            STATUS   ROLES    AGE     VERSION
control-plane   Ready    master   26m     v1.16.3
worker-node1    Ready    <none>   3m18s   v1.16.3

$ kubectl get nodes -o wide
--> this will display IP, OS, Kernel and more details about all Nodes

#
Project-1 [Nginx]
#############################################
	Deploying/Creating a pod
#############################################

#
1.) Create Pod manifest file
$ mkdir nginx
$ vim pod.yaml

pod.yaml
=========
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
  labels:
    env: prod
    version: v1.2.3
spec:
  containers:
  - name: nginx-container
    image: nginx
    ports:
    - containerPort: 80

	

pod.yaml - Manifest file description:
----------------------
- Straight away we can see four top-level resources.
• .apiVersion
• .kind
• .metadata
• .spec

--> .apiVersion:
	- Tells API Server about what version of Yaml is used to create the object (Pod object in this case)
	- Pods are currently in v1 API group

--> .kind:
	- Tells us the kind of object being deployed. In this case we are creating POD object.
	- It tells control plane what type of object is being defined.

--> .metadata:
	- this section again has two sub-sections i.e name & labels
	- You can name the Pod using "name" key.
	- Using labels, we can identify a particular pod.

--> .spec:
	- This is where we specify details about the containers that will run in the Pod.
	- In this section we specify container name, image, ports ..etc.

#
2.) Creating a Pod
- Check if all Nodes are ready before creating a Pod
$ kubectl get nodes

- This POSTs the manifest file to API server and deploy/create a Pod from it
$ kubectl apply -f pod.yml
Note: Your Pod has been scheduled to a healthy node in the cluster and
is being monitored by the local kubelet process on the node.

# Introspecting Running Pods
- Get IP and worker node of the Pod
$ kubectl get pod -o wide


- Launch nginx server application running in the Pod from Controle-plane node
$ curl http://10.44.0.1:80
$ curl http://POD-IP:Server-Port


- You can also login into the Pod container to get more information.
$ kubectl exec nginx-app -it -- /bin/bash
	Note: Let's add some code and launch our nginx application
	- $ echo "Wiculty Learning Solutions" > /usr/share/nginx/html/index.html 

- Launch nginx application
$ curl http://10.44.0.1:80

- Login into a specific container in case you have multi container Pod
  using --container or -c option.

$ kubectl exec nginx-app -c container-name -it -- /bin/bash

#
3.) Deleting a Pod
$ kubectl get pods
$ kubectl delete pods nginx-pod
$ kubectl delete -f pod.yml

--POD--

NOTE:
kubelet takes the PodSpec and is responsible for pulling all images and starting all containers in the Pod.

What Next?
- If a Pod fails, it is not automatically rescheduled. Because of this, we usually deploy
them via higher-level object such as Deployments.

- This adds things like "scalability" (scale-up/down), "self-healing", "rolling updates" and "roll backs" and makes Kubernetes so powerful.


Misc. CMDs:
- Get full copy of the Pod manifest from cluster store. desired state is (.spec) and oberved state will be under (.status)
$ kubectl get pod -o yaml

- Check if Pod is created
$ kubectl get pods
$ kubectl get pods --watch (monitor the status continuously)


- Another great Kubernetes introspection command. Provides Pods(object's) lifecycle events.
$ kubectl describe pod nginx-pod

#
Project-1 [Nginx]
#############################################
	Creating Deployments & Services
#############################################

- Pods don’t self-heal, they don’t auto-scale, and they don’t allow for easy updates.

- So we use K8S Deployment Object to create the Pods in real-time. If we create the Pods using Deployment Object, we get below advantages..
	- Auto scale (You can scalp up and scale down the pods very easily)
	- Self-heal (Pods can be self-healed automcatically)
	- Rolling updates (You can deploy the new code / new image very easily if Pods are created using Deployments )
	- Roll backs (Role-back become very easy as you can role-back the deployment instead of rolling-back Pods one by one)
	- Creating End-pod (LB URL) of application: Pods can be exposed very easily as you just need to give deployment to the Service 
	  instead adding Pod's one by one to LB/Service.
	
- That's why we almost always deploy Pods via 'Deployments"

# Test Rolling Updates
kubectl set image deployments/nginx-dep nginx-c=nageshvkn/gamutkart-imgcamp:v2

# Test Rollback


#
Creating Deployments
--------------------
# List all nodes in K8s cluster
$ kubectl get nodes

# List all pods in K8s cluster
$ kubectl get pods

# Create the deployment
$ kubectl apply -f deploy-nginx.yml

vim deploy-nginx.yml
-------
apiVersion: apps/v1
kind: Deployment
metadata:
    name: nginx-prod-deploy

spec:
  replicas: 6
  selector:
    matchLabels:
      app: nginx-pod
  template:
    metadata:
      labels:
        app: nginx-pod
    spec:
      containers:
        - name: nginx-container
          image: nginx
          ports:
          - containerPort: 80

		  
# Creating deployment
$ kubectl apply -f deploy-nginx.yml

# Check pod creations
$ kubectl get pods --watch

# Login to pods and verify nginx application
$ kubectl get pods -o wide
$ kubectl exec -it nginx-deploy-5f654bcccd-27xtg /bin/bash
	
# launch application from individual Pod
$ curl http://10.44.0.1:80
$ curl http://pod_ip:80

-->describe

# Testing Self-healing capability
----------------------
If you delete some Pods, Kubernetes can automatically re-create the same for us to make sure given no. of Pods are always running.

- Delete the Pods
$ kubectl delete pods POD_NAME1 POD_NAME2

- Check if the Pods are re-created
	$ kubects get pods

--
Creating service to expose the application to outside world and setting up load balancer
===============================================
$ vim service-nginx.yml
-----
apiVersion: v1
kind: Service
metadata:
    name: nginx-prod-service
    labels:
      app: nginx-app-prod-service
spec:
  selector:
    app: nginx-pod

  type: NodePort

  ports:
  - nodePort: 31000
    port: 80
    targetPort: 80

# Create the service
$ kubectl create -f service-nginx.yml

# Enable networking
Click on Navigation menu(three lines on top left) --> Go to VPC Network --> Firewal rules --> select on one existing rule --> edit --> Source IP ranges
--> 0.0.0.0/0 --> In 'Specified protocols and ports', write this range "31000"

# Access the application from browser using worker-node port
http://34.93.139.52:31000/
http://WorkerNodeIP:NodePort


Project-2 [GamutKart]
==========================
# Creating deployment for GamutKart
$ vim deploy-gamutkart.yml
apiVersion: apps/v1
kind: Deployment
metadata:
    name: gamutkart-deploy
    labels:
      app: gamutkart-app
spec:
  replicas: 8
  selector:
    matchLabels:
      app: gamutkart-app
  template:
    metadata:
      labels:
        app: gamutkart-app
    spec:
      containers:
        - name: gamutkart-container
          image: nageshvkn/gamutkart-img-k8s
	  resources:
      	    requests:
              memory: "64Mi"
              cpu: "250m"
            limits:
              memory: "128Mi"
              cpu: "500m"
          ports:
          - containerPort: 8080
          command: ["/bin/sh"]
          args:  ["-c", "/root/apache-tomcat-8.5.38/bin/startup.sh; while true; do sleep 1; done;"]
	
# Execute 	
$ kubectl apply -f  deploy-gamutkart.yml

# Creating service for GamutKart
$ vim service-gamutkart.yml

apiVersion: v1
kind: Service
metadata:
    name: gamutkart-service
    labels:
      app: gamutkart-app
spec:
  selector:
    app: gamutkart-app
  type: LoadBalancer
  ports:
  - nodePort: 31000
    port: 8080
    targetPort: 8080
	

# Creating the service
$ kubectl apply -f service-gamutkart.yml


#
# Enable networking
TODO:
Go to VPC Network --> Firewal --> select on one existing rule --> edit --> Source IP ranges
--> 0.0.0.0/0 --> In "Specified protocols and ports", write this range "0-65535"

Note:
Kubernates Port Range: 30,000 - 32,767


#
3.) Deleting a Pod
$ kubectl get pods
$ kubectl delete pods nginx-pod
$ kubectl delete -f pod.yml

# Misc:
4.) Get all nodes IPs in Kubernetes cluster
$ kubectl get nodes -o wide

#
List Deployments & Service
$ kubectl get deployment
$ kubectl get svc (Or service)

#
5.) Deleting Deployment & Service
$ kubectl delete -f deploy-gamutkart.yaml(deployment yaml file name)
$ kubectl delete -f service-gamutkart.yml( service yaml file name)

$ kubectl delete deployment <deployment-name>
$ kubectl delete service <service-name>


#Scaleup Pods
$ kubectl scale deployment/gamutkart-deploy --replicas=2


# Misc:
===============
1. List all the pods which are under a Service

--> Describe the service & find the Pod's Label which are tied up with the service first. In below case it is "app=nginx-pod".
	$ kubectl describe service <service-name> (check for "Selector: app=nginx-pod in the output)
")
--> List all Pods which have the label. Example, in above case it is: app=nginx-pod.
	$ kubectl get pods  -l app=nginx-pod

Autoscaling
=================
1. Autoscale Pods/Deployments
--
# How do you autoscale the deployment?
$ kubectl autoscale deployment <deployment-name> --max=4 --cpu-percent=70

- The above command, maximum creates 4 instances of your application and autoscales the instances whenever CPU utilisation touches to 70% threshold.

- When you run the autoscale command, internally it creates "Horizontal Pod Autoscaler" (HPA). HPA takes care of autoscaling activity.Check it using below command.
$ kubectl get hpa

2. Autoscale Kubernetes Cluster
--
- We need to also have enough number of Worker nodes to supply compute resources (RAM/CPU) to Pods as Pods use WorkerNode's compute. In GKE cluster, Google cloud automatically takes care of WorkerNode's auto scaling. However, if you have a specific requirements, you can set up WorkerNode autoscaling.

- So, using bellow command, you can autoscale WorkerNodes.
$ gcloud container clusters update cluster-name --enable-autoscaling --min-nodes=1 --max-nodes=10


=====
How to validate Yaml

=====
5. Helm Charts

- Stateless Vs State-full applications
- PV & PVC
- StatefullSet

3. ZDR - Rolling update

- Namespace

8. Roleback

1. Secret to connect to private or any repository
2. ConfigMaps

8. Ingress


CKA



















